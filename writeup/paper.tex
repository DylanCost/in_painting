\documentclass{article}
\usepackage{iclr2026_conference,times}  

% Optional math commands
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
 
\title{A Comparative Study of Generative Models for \\Facial Image Inpainting: Diffusion, VAE, and Flow Matching}

% Authors hidden for double-blind review
\author{Dylan Costello, Noah Giles, Nicholas Cox}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

% use a neutral header instead of the ICLR one
\lhead{A Comparative Study of Generative Models for Facial Image Inpainting}


\begin{abstract}
Image inpainting, the task of reconstructing missing or corrupted regions in images, remains a fundamental challenge in computer vision. We present a systematic comparison of three generative paradigms for facial image inpainting: Diffusion Models, Variational Autoencoders (VAEs), and Flow Matching. To ensure a fair evaluation, we conduct all comparisons within a controlled experimental framework that uses U-Net-style architectures with comparable parameter counts, consistent training configurations, and standardized mask generation strategies on the CelebA dataset. Under a fixed training budget, our experiments indicate that VAEs achieve the strongest overall reconstruction quality, while Flow Matching provides competitive performance with greater training stability than diffusion. Diffusion models, despite their recent success in generative tasks, exhibit significant training instability in this constrained setting, and we hypothesize that both diffusion and flow matching would benefit from expanded compute budgets (e.g., longer training horizons or larger architectures) for inpainting tasks.
\end{abstract}

\section{Introduction}

Image inpainting---the reconstruction of missing or damaged regions within an image---is a long-standing problem in computer vision with applications spanning photo restoration, object removal, and content-aware editing. The advent of deep generative models has dramatically advanced the state of the art, enabling semantically coherent reconstructions that respect both local texture patterns and global image structure.

Among generative approaches, three paradigms have emerged as particularly influential: Variational Autoencoders \citep{kingma2014auto}, which learn compressed latent representations through variational inference; Diffusion Models \citep{ho2020denoising}, which generate samples through iterative denoising; and Flow Matching \citep{lipman2023flow}, which learns continuous normalizing flows via regression to target velocity fields. Each paradigm offers distinct inductive biases and training dynamics that may differentially impact inpainting performance.

Facial inpainting presents an ideal benchmark for comparing these approaches. Human faces exhibit strong structural regularities (symmetric features, consistent spatial relationships) while also requiring fine-grained texture synthesis (skin, hair, eyes). This combination of global coherence and local detail makes facial inpainting a discriminating test of generative model capabilities.

Despite the proliferation of generative approaches, direct comparisons remain challenging due to architectural differences, varying training protocols, and inconsistent evaluation settings, which make it difficult to isolate the contribution of the generative paradigm from confounding factors.

In this work, we perform a controlled comparison of Diffusion Models, VAEs, and Flow Matching for facial inpainting. Our experimental design emphasizes fairness through several key decisions: (1) closely matched U-Net-style architectures with similar parameter counts across all models (2) identical data preprocessing and mask generation procedures, (3) consistent training hyperparameters and compute budgets, and (4) evaluation on the same held-out test set using standard metrics.

\section{Related Work}

Variational Autoencoders \citep{kingma2014auto, rezende2014stochastic} learn latent variable models through amortized variational inference. The encoder maps inputs to a distribution over latent codes, while the decoder generates outputs conditioned on sampled latents. Training maximizes a variational lower bound comprising reconstruction and KL-divergence terms. VAEs have been successfully applied to inpainting by conditioning on masked inputs and reconstructing complete images, albeit with limited diversity in reconstructions \citep{zheng2019pluralistic}.

Denoising Diffusion Probabilistic Models \citep{ho2020denoising, sohl2015deep} define a forward process that gradually corrupts data with Gaussian noise and learn a reverse process that iteratively denoises to recover clean samples. \citet{song2021scorebased} unified score-based and diffusion perspectives, while \citet{dhariwal2021diffusion} demonstrated that diffusion models can surpass GANs on image synthesis. For inpainting, diffusion models can be conditioned on unmasked regions through modified sampling procedures \citep{lugmayr2022repaint}.

Flow Matching \citep{lipman2023flow} and related approaches \citep{liu2023flow, albergo2023building} learn continuous normalizing flows by regressing to target velocity fields. Unlike traditional normalizing flows, flow matching avoids expensive Jacobian computations during training. Rectified flows \citep{liu2023flow} further simplify the target velocity to straight-line interpolations between noise and data. These methods have shown promise for efficient high-quality generation.

The U-Net architecture \citep{ronneberger2015u} employs an encoder-decoder structure with skip connections that concatenate encoder features to corresponding decoder layers. Originally developed for biomedical image segmentation, U-Net has become the de facto standard for image-to-image tasks including inpainting and forms the backbone of most diffusion model implementations.

\section{Methods}

\subsection{Shared Architecture and Training Setup}

To reduce architectural confounds, we base all three models on U-Net-style encoder--decoder backbones with the same depth and channel layout, though we do not enforce a perfectly identical implementation across paradigms. Each network uses a series of encoder and decoder blocks with channel dimensions $[64, 128, 256, 512, 512]$, with two convolutional layers per block, BatchNorm, GELU activations, and skip connections. The placement and use of attention blocks is not fully shared: the flowmatching and diffusion implementations include one attention block in the encoder and decoder components, while the VAE additionally applies attention at multiple resolution levels.

All models receive as input a 4-channel tensor: the 3-channel masked image (with zeros in masked regions) concatenated with a 1-channel binary mask indicating missing pixels. This conditioning strategy provides explicit information about which regions require reconstruction.

We keep most high-level training settings aligned across models: all experiments use a batch size of 64, are run for 100 epochs, and are evaluated on the validation set every epoch, reporting test-set metrics for the best validation checkpoint. However, the precise optimizer settings and learning rate schedules differ between paradigms; Table~\ref{tab:lr_schedules} summarizes the starting and ending learning rates and warmup policies for each model.

\begin{table}[t]
\centering
\caption{Learning rate schedules and warmup policies for each model. All learning rates use cosine annealing after any warmup phase.}
\label{tab:lr_schedules}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Optimizer} & \textbf{LR schedule} & \textbf{Warmup} \\
\midrule
VAE & Adam & $2\times 10^{-4} \rightarrow 10^{-6}$ over 100 epochs & None \\
Diffusion & AdamW & $2\times 10^{-4} \rightarrow 10^{-6}$ over 100 epochs & None \\
Flow Matching & AdamW & $1\times 10^{-4} \rightarrow 10^{-5}$ over training steps & Linear, first 1000 steps \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Diffusion Model Implementation}

Our diffusion model follows the DDPM formulation \citep{ho2020denoising}. The forward process adds Gaussian noise according to a linear schedule with $\beta_t$ ranging from $10^{-4}$ to $0.02$ over $T=1000$ timesteps:
\begin{equation}
    q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t \mathbf{I})
\end{equation}

The U-Net is trained to predict the noise $\epsilon$ added at each timestep, conditioned on the masked input and mask. The training objective is:
\begin{equation}
    \mathcal{L}_{\text{diffusion}} = \mathbb{E}_{t, x_0, \epsilon}\left[ \| \epsilon - \epsilon_\theta(x_t, t, x_{\text{masked}}, m) \|^2 \right]
\end{equation}
where $x_{\text{masked}}$ is the masked image and $m$ is the binary mask.

At inference, we perform $T$ denoising steps starting from pure noise in masked regions. At each timestep $t$, we sample using:
\begin{equation}
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z
\end{equation}
where $z \sim \mathcal{N}(0, \mathbf{I})$ for $t > 1$ and $z = 0$ for $t = 1$, $\alpha_t := 1 - \beta_t$, and $\bar{\alpha}_t := \prod_{s=1}^{t} \alpha_s$. This progressively denoises the image by subtracting scaled noise predictions at each step.


\subsection{VAE Implementation}

Our VAE adapts the U-Net backbone into an encoder-decoder structure with a latent bottleneck. The encoder maps the concatenated masked image and mask to mean $\mu$ and log-variance $\log\sigma^2$ vectors of dimension 512. We apply the reparameterization trick to sample latent codes:
\begin{equation}
    z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})
\end{equation}

The decoder reconstructs the full image from the latent code. The training objective combines reconstruction and regularization:
\begin{equation}
    \mathcal{L}_{\text{VAE}} = \| x - \hat{x} \|_1 + \beta \cdot D_{\text{KL}}(q(z|x) \| p(z))
\end{equation}
where $\beta=0.001$ balances reconstruction fidelity against latent regularization. We use L1 reconstruction loss for sharper outputs.

Unlike diffusion and flow matching, the VAE produces reconstructions in a single forward pass, offering significant inference speed advantages.

\subsection{Flow Matching Implementation}

Our flow matching implementation follows \citet{lipman2023flow} and \citet{liu2023flow}. We define an interpolation between source (Gaussian noise) and target (ground truth) distributions:
\begin{equation}
    x_t = (1-t) \cdot \epsilon + t \cdot x_0, \quad t \in [0, 1]
\end{equation}

The U-Net learns to predict the velocity field $v_t$ that transports probability mass along this interpolation:
\begin{equation}
    v_t^* = x_0 - \epsilon
\end{equation}

The training objective regresses to this target velocity:
\begin{equation}
    \mathcal{L}_{\text{flow}} = \mathbb{E}_{t, x_0, \epsilon}\left[ \| v_\theta(x_t, t, x_{\text{masked}}, m) - v_t^* \|^2 \right]
\end{equation}

At inference on the test set, we integrate the learned velocity field from $t=0$ to $t=1$ using 100 Euler steps, initializing from Gaussian noise in masked regions.

\section{Dataset}

\subsection{CelebA Dataset}

We conduct our experiments on CelebA (CelebFaces Attributes) \citep{liu2015deep}, a large-scale face attributes dataset containing over 200,000 celebrity images with rich pose variation and background clutter. CelebA is widely adopted for facial analysis tasks including inpainting, making it suitable for benchmarking generative models.

For computational efficiency and to maintain reasonable training times on consumer hardware, we resize all images to 128$\times$128 pixels and normalize pixel values to the range $[-1, 1]$. We start from the official CelebA split sizes---162,770 training, 19,867 validation, and 19,962 test images---but in our experiments we use only the first 1,024 images from the validation split for validation and fold the remaining 18,843 validation images into the training set, yielding 181,613 training images, 1,024 validation images, and 19,962 test images.

\subsection{Mask Generation}

To simulate realistic inpainting scenarios while maintaining experimental control, we employ random rectangular masks with side lengths uniformly sampled from $[16, 64]$ pixels. Mask positions are sampled uniformly across the image, ensuring that masks may occlude any facial region including eyes, nose, mouth, or hair.

Critically, we generate a fixed set of mask-image pairs for evaluation, ensuring that all models are assessed on identical inpainting challenges. This eliminates variance due to mask sampling and enables direct comparison of reconstruction quality.

\section{Results}\label{sec:experimental_results}


We evaluate reconstruction quality using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Absolute Error (MAE).
All metrics are computed over the masked regions only, as unmasked pixels are trivially preserved.

\begin{table}[t]
\caption{Test set performance comparison at epoch 100. Best results in bold.}
\label{tab:results}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{PSNR (dB) $\uparrow$} & \textbf{SSIM $\uparrow$} & \textbf{MAE $\downarrow$} \\
\midrule
VAE & \textbf{25.57} & \textbf{0.636} & \textbf{0.103} \\
Flow Matching & 18.1 & 0.541 & 0.148 \\
Diffusion & 17.9 & 0.523 & 0.161 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Table~\ref{tab:results} summarizes the final test set performance for each model. In addition to these aggregate metrics, Appendix~\ref{sec:qualitative_examples} presents qualitative triptych examples for each model, with Flow Matching reconstructions illustrated in Figures~\ref{fig:fm_triptych_panel_01} and \ref{fig:fm_triptych_panel_02}. Diffusion reconstructions are illustrated in  Figures~\ref{fig:diff_triptych_panel_01} and \ref{fig:diff_triptych_panel_02}.

\subsection{Training Dynamics}

Beyond final performance, the training curves reveal important differences in optimization behavior (Figures~\ref{fig:psnr}, \ref{fig:ssim}, \ref{fig:mae}).

\textbf{VAE} exhibits remarkably stable training with nearly monotonically improving metrics throughout. PSNR increases smoothly from 19 dB to about 25 dB, while SSIM converges to 0.636 with minimal variance. We conjecture that this stability reflects the well-behaved optimization landscape of the VAE objective.

\textbf{Flow Matching} also demonstrates stable training dynamics, with PSNR improving from 11 dB to 18 dB and SSIM from 0.19 to 0.54. The relatively smooth convergence curves suggest that flow matching provides reliable gradients throughout training.

\textbf{Diffusion} exhibits pronounced instability, with large spikes in all metrics throughout training. PSNR fluctuates dramatically, dropping to below 5 dB at certain epochs before recovering. Similarly, MAE shows spikes exceeding 0.10. This likely reflects the stochastic nature of the diffusion process, where the inherently noisy training objective can introduce significant variance in the optimization landscape.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/PSNR_comparison.png}
\end{center}
\caption{Validation PSNR over training epochs.}
\label{fig:psnr}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/SSIM_comparison.png}
\end{center}
\caption{Validation SSIM over training epochs.}
\label{fig:ssim}
\end{figure}

\begin{figure}[t] 
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/MAE_comparison.png}
\end{center}
\caption{Validation MAE over training epochs.}
\label{fig:mae}
\end{figure}

\section{Discussion}

The substantial performance gap between VAE and the other approaches warrants examination. We hypothesize several contributing factors:

\textbf{Direct reconstruction objective.} The VAE directly optimizes reconstruction loss, whereas diffusion and flow matching learn auxiliary objectives (noise prediction and velocity regression) that indirectly enable generation. For the specific task of inpainting---where the goal is reconstruction rather than diverse sampling---direct optimization may be advantageous.

\textbf{Single-pass inference.} VAE reconstruction requires only one forward pass, while diffusion and flow matching require iterative sampling. With fixed training epochs, the VAE processes more effective gradient updates per epoch since each sample is used once for direct reconstruction rather than across multiple timesteps.

\textbf{Training horizon.} Diffusion models are known to require extensive training to achieve optimal performance. Our 100-epoch budget, while sufficient for VAE convergence, may be inadequate for diffusion models to fully learn the score function across all noise levels.

\textbf{Conditioning mechanism.} Our simple concatenation-based conditioning may not optimally leverage the masked image information for iterative models. More sophisticated conditioning approaches (e.g., cross-attention, guided sampling) could improve diffusion and flow matching performance.

The instability of diffusion model training is particularly notable. The large metric fluctuations suggest sensitivity to the noise level distribution during training or potential mode-seeking behavior. Techniques such as loss weighting across timesteps \citep{ho2020denoising} or progressive training schedules may help stabilize optimization.

\section{Conclusion}

We conducted a controlled comparison of Diffusion Models, Variational Autoencoders, and Flow Matching for facial image inpainting on CelebA. By aligning architectures, training protocols, and evaluation masks, our study isolates the role of the generative formulation itself, rather than differences in model capacity or experimental setup.

Within this constrained regime, VAEs emerge as the most effective choice for reconstruction-focused facial inpainting: they deliver consistently strong completions while training in a stable and data-efficient manner, and they offer fast, single-pass inference. Flow Matching provides a competitive alternative that combines stable optimization with conceptually simple continuous flows, positioning it as a promising middle ground between VAEs and diffusion-based approaches. In contrast, our diffusion implementation struggles to realize its potential under the limited training budget and simple conditioning scheme considered here, exhibiting highly variable training dynamics and weaker reconstructions.

These outcomes highlight an important practical message: for moderately sized facial inpainting problems with tight compute budgets and straightforward masking, classical latent-variable models remain highly competitive, and in many cases preferable, to more recent iterative generative methods. At the same time, our design choices intentionally favor clarity and control over exhaustiveness. Extensions such as richer mask geometries, higher resolutions, stronger architectural priors, and more expressive conditioning mechanisms, particularly for diffusion and flow matching, may shift the balance in future studies, and exploring these regimes will be essential for fully characterizing the trade-offs between these paradigms.

\bibliography{references}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Implementation Details}

\subsection{Differences between the U-Net variants}

All three models share a 5-level U-Net backbone with channel progression
$[64, 128, 256, 512, 512]$ and an input resolution of $128\times128$, but they
are instantiated differently depending on the generative objective (flow
matching, diffusion, or variational autoencoding). We summarize the key
architectural differences below.

\begin{enumerate}
\item \emph{Flow-matching U-Net.}
The flow-matching model uses a time-conditioned U-Net that directly predicts a
3-channel velocity field on RGB space. The network takes a concatenated
image--mask tensor with $4$ input channels (RGB $+$ binary mask) and a
continuous flow time $t\in[0,1]$. Time is embedded by a sinusoidal
positional encoder followed by an MLP, and the resulting vector is injected
into every encoder and decoder block. Self-attention is applied only at a
single intermediate resolution (default $16\times16$), selected by matching
spatial resolution rather than layer depth, and mirrored at the corresponding
decoder level. Skip connections are always enabled between encoder and
decoder stages. The output is a full-frame $3$-channel velocity field; any
restriction to the inpainted region is enforced by the flow-matching loss and
ODE sampler rather than by masking the U-Net output.

\item \emph{Diffusion U-Net.}
The diffusion model employs a similar time-conditioned U-Net whose role is
to predict the additive noise in the masked region during the denoising
process. The configuration shared with the flow-matching model specifies
$3$ RGB channels and hidden dimensions $[64,128,256,512,512]$, with self-attention
again enabled only at the $16\times16$ scale and mirrored in the decoder.
However, the diffusion U-Net is conditioned on \emph{integer} diffusion
timesteps $t\in\{0,\dots,T-1\}$ via a sinusoidal embedding and small MLP, and
it treats the inpainting mask in two distinct ways: (i) the binary mask is
concatenated to the image as an additional input channel before the encoder,
and (ii) the final $3$-channel noise prediction is multiplied by the mask
(channel-wise), so that the network is explicitly constrained to predict
non-zero noise only inside the inpaint region. Thus, unlike the flow-matching
variant, the diffusion U-Net both conditions on the mask and hard-enforces it
at the output.

\item \emph{VAE U-Net.}
The VAE inpainting model uses a U-Net-based variational autoencoder with the same 5-level
encoder--decoder backbone but without any time or diffusion-step conditioning.
The encoder consumes a masked image and an optional binary mask by
concatenating them into a $(3{+}1)$-channel input; the mask therefore
influences the latent representation but is \emph{not} provided to the
decoder separately. The encoder outputs are flattened and projected into mean and
log-variance vectors in a low-dimensional latent space (default dimension
$32$), and the decoder reconstructs an RGB image from a sampled latent code,
using U-Net skip connections (when enabled) and self-attention applied at the
last two encoder stages and the mirrored first two decoder stages. The final
reconstruction head applies a $\tanh$ nonlinearity, yielding a $3$-channel
image in $[-1,1]$ without any explicit masking at the output.
\end{enumerate}

\subsection{Hyperparameters}

Table~\ref{tab:hyperparams} summarizes the hyperparameters used across all experiments.

\begin{table}[h]
\caption{Training hyperparameters.}
\label{tab:hyperparams}
\begin{center}
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Image resolution & 128$\times$128 \\
Batch size & 64 \\
Training epochs & 100 \\
Mask size range & [16, 64] pixels \\
\midrule
\multicolumn{2}{c}{\textit{Diffusion-specific}} \\
\midrule
Diffusion steps $T$ & 1000 \\
$\beta$ schedule & Linear ($10^{-4}$ to 0.02) \\
Time embedding dimension & 256 \\
\midrule
\multicolumn{2}{c}{\textit{VAE-specific}} \\
\midrule
Latent dimension & 32 \\
KL weight $\beta$ & 0.001 \\
\midrule
\multicolumn{2}{c}{\textit{Flow Matching-specific}} \\
\midrule
Integration steps & 100 \\
Time embedding dimension & 256 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Additional Qualitative Inpainting Examples}
\label{sec:qualitative_examples}

To complement the quantitative results in Section~\ref{sec:experimental_results}, we include qualitative triptych visualizations of the inpainting behavior of each model. Each triptych shows the masked input image, the ground-truth target, and the model's reconstruction.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/fm_triptych_panel_01.png}
\end{center}
\caption{Qualitative inpainting examples from the Flow Matching model (panel 1). Each triptych shows the ground-truth target (top), masked input (middle), and Flow Matching reconstruction (bottom).}
\label{fig:fm_triptych_panel_01}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/fm_triptych_panel_02.png}
\end{center}
\caption{Qualitative inpainting examples from the Flow Matching model (panel 2), highlighting reconstructions under varying mask placements and occlusion severities.}
\label{fig:fm_triptych_panel_02}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/diff_triptych_panel_01.png}
\end{center}
\caption{Qualitative inpainting examples from the Diffusion model (panel 1). Each has the original image on the first row, masked on the second row, and the inpainted version on the third row}
\label{fig:diff_triptych_panel_01}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/diff_triptych_panel_02.png}
\end{center}
\caption{Qualitative inpainting examples from the Diffusion model (panel 2)}
\label{fig:diff_triptych_panel_02}
\end{figure}

\end{document}
